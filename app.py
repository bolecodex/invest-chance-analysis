"""
å•†æœºå‘æ˜ â€” å¤šæ™ºèƒ½ä½“æœ€å° Demo
============================
è¿è¡Œæ–¹å¼: python app.py
è®¿é—®åœ°å€: http://localhost:8000
"""
from __future__ import annotations

import asyncio
import json
import os
import time
import uuid
from contextlib import asynccontextmanager
from datetime import datetime
from typing import Any, Optional

from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

load_dotenv()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                        é…ç½® & å…¨å±€çŠ¶æ€                          â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
USE_MOCK = not OPENAI_API_KEY or OPENAI_API_KEY in ("sk-your-key-here", "your-ark-api-key-here")

# è”ç½‘é—®ç­”æ™ºèƒ½ä½“é…ç½®
WEB_AGENT_API_KEY = os.getenv("WEB_AGENT_API_KEY", "")
WEB_AGENT_BOT_ID = os.getenv("WEB_AGENT_BOT_ID", "7563185232485123593")
WEB_AGENT_URL = "https://open.feedcoopapi.com/agent_api/agent/chat/completion"
USE_WEB_AGENT = bool(WEB_AGENT_API_KEY) and WEB_AGENT_API_KEY != "your-web-agent-api-key-here"

# å†…å­˜æ•°æ®åº“
db: dict[str, list] = {
    "articles": [],
    "cleaned": [],
    "opportunities": [],
    "reports": [],
    "pipeline_runs": [],
}


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                        æ ·æœ¬æ–‡ç« æ•°æ®                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SAMPLE_ARTICLES = [
    {
        "id": "art-001",
        "title": "æœˆä¹‹æš—é¢å®Œæˆè¶…10äº¿ç¾å…ƒèèµ„ï¼Œä¼°å€¼çº¦33äº¿ç¾å…ƒ",
        "author": "36æ°ª",
        "publish_time": "2026-01-15",
        "url": "https://example.com/article/001",
        "content": """
        æ®36æ°ªç‹¬å®¶è·æ‚‰ï¼Œå¤§æ¨¡å‹åˆ›ä¸šå…¬å¸æœˆä¹‹æš—é¢ï¼ˆMoonshot AIï¼‰å·²äºè¿‘æœŸå®Œæˆè¶…10äº¿ç¾å…ƒèèµ„ï¼Œ
        æŠ•åä¼°å€¼çº¦33äº¿ç¾å…ƒã€‚æœ¬è½®èèµ„ç”±çº¢æ‰ä¸­å›½ã€å°çº¢ä¹¦é¢†æŠ•ï¼Œé˜¿é‡Œå·´å·´ã€ç¾å›¢ã€è“é©°åˆ›æŠ•ç­‰è·ŸæŠ•ã€‚

        æœˆä¹‹æš—é¢æˆç«‹äº2023å¹´3æœˆï¼Œåˆ›å§‹äººæ¨æ¤éºŸæ¯•ä¸šäºæ¸…åå¤§å­¦è®¡ç®—æœºç³»ï¼Œæ›¾åœ¨å¡å†…åŸºæ¢…éš†å¤§å­¦
        (CMU) æ”»è¯»åšå£«å­¦ä½ï¼Œå¸ˆä»è‹¹æœAIè´Ÿè´£äºº Ruslan Salakhutdinovã€‚æ¨æ¤éºŸæ›¾åœ¨ Google Brain
        å®ä¹ ï¼Œå‘è¡¨è¿‡å¤šç¯‡ NeurIPSã€ICML é¡¶ä¼šè®ºæ–‡ï¼Œå…¶ä»£è¡¨ä½œ Transformer-XL å’Œ XLNet è®ºæ–‡
        å¼•ç”¨é‡åˆè®¡è¶…è¿‡8000æ¬¡ã€‚

        å…¬å¸æ ¸å¿ƒäº§å“ä¸º Kimi æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸»æ‰“è¶…é•¿ä¸Šä¸‹æ–‡çª—å£èƒ½åŠ›ï¼ˆæ”¯æŒ200ä¸‡å­—è¾“å…¥ï¼‰ï¼Œä¸Šçº¿ä»¥æ¥
        æœˆæ´»ç”¨æˆ·å·²çªç ´1200ä¸‡ã€‚å›¢é˜Ÿç›®å‰çº¦200äººï¼Œæ ¸å¿ƒæˆå‘˜æ¥è‡ªæ¸…åå¤§å­¦ã€Google Brainã€Meta AI
        ç­‰é¡¶çº§æœºæ„ã€‚

        æœˆä¹‹æš—é¢æ­¤å‰å·²å®Œæˆå¤©ä½¿è½®ï¼ˆ2023å¹´6æœˆï¼Œçº¦2000ä¸‡ç¾å…ƒï¼Œçº¢æ‰ä¸­å›½é¢†æŠ•ï¼‰å’ŒAè½®ï¼ˆ2023å¹´12æœˆï¼Œ
        çº¦10äº¿ç¾å…ƒï¼Œå¤šå®¶æœºæ„è”åˆæŠ•èµ„ï¼‰ã€‚æ­¤è½®èèµ„åï¼Œæœˆä¹‹æš—é¢å°†æˆä¸ºå›½å†…ä¼°å€¼æœ€é«˜çš„å¤§æ¨¡å‹åˆ›ä¸š
        å…¬å¸ä¹‹ä¸€ï¼Œèµ„é‡‘å°†ä¸»è¦ç”¨äºæ¨¡å‹è®­ç»ƒç®—åŠ›é‡‡è´­å’Œäº§å“ç ”å‘ã€‚
        """,
    },
    {
        "id": "art-002",
        "title": "äº‘åŸç”Ÿå®‰å…¨å…¬å¸æ¢çœŸç§‘æŠ€å®ŒæˆAè½®æ•°åƒä¸‡å…ƒèèµ„",
        "author": "æŠ•èµ„ç•Œ",
        "publish_time": "2026-01-20",
        "url": "https://example.com/article/002",
        "content": """
        æŠ•èµ„ç•Œ1æœˆ20æ—¥æ¶ˆæ¯ï¼Œäº‘åŸç”Ÿå®‰å…¨å…¬å¸æ¢çœŸç§‘æŠ€è¿‘æ—¥å®£å¸ƒå®Œæˆæ•°åƒä¸‡å…ƒAè½®èèµ„ï¼Œæœ¬è½®èèµ„ç”±
        ç»çº¬åˆ›æŠ•é¢†æŠ•ï¼Œè€è‚¡ä¸œçº¢ç‚¹ä¸­å›½è·ŸæŠ•ã€‚èèµ„èµ„é‡‘å°†ä¸»è¦ç”¨äºäº§å“ç ”å‘å’Œå¸‚åœºæ‹“å±•ã€‚

        æ¢çœŸç§‘æŠ€æˆç«‹äº2024å¹´åˆï¼Œä¸“æ³¨äºäº‘åŸç”Ÿç¯å¢ƒä¸‹çš„å®‰å…¨æ£€æµ‹ä¸é˜²æŠ¤ã€‚å…¬å¸åˆ›å§‹äººå…¼CEOææ˜è¾‰
        æ‹¥æœ‰15å¹´ç½‘ç»œå®‰å…¨ä»ä¸šç»éªŒï¼Œæ›¾ä»»é˜¿é‡Œäº‘å®‰å…¨äº§å“çº¿è´Ÿè´£äººï¼Œæ­¤å‰åœ¨ç»¿ç›Ÿç§‘æŠ€æ‹…ä»»é«˜çº§ç ”ç©¶å‘˜ã€‚
        CTO ç‹ç£Šä¸ºæ¸…åå¤§å­¦ç½‘ç»œå®‰å…¨æ–¹å‘åšå£«ï¼Œæ›¾åœ¨ IEEE S&Pã€USENIX Security ç­‰é¡¶çº§å®‰å…¨
        ä¼šè®®ä¸Šå‘è¡¨å¤šç¯‡è®ºæ–‡ã€‚

        å…¬å¸æ ¸å¿ƒäº§å“"æ¢çœŸäº‘å«"æ˜¯ä¸€æ¬¾é¢å‘ Kubernetes ç¯å¢ƒçš„å®‰å…¨å¹³å°ï¼Œæä¾›å®¹å™¨é•œåƒæ‰«æã€
        è¿è¡Œæ—¶å¨èƒæ£€æµ‹ã€åˆè§„å®¡è®¡ç­‰åŠŸèƒ½ã€‚ç›®å‰å·²æœåŠ¡è¶…è¿‡50å®¶ä¼ä¸šå®¢æˆ·ï¼ŒåŒ…æ‹¬å¤šå®¶é‡‘èæœºæ„å’Œ
        äº’è”ç½‘å…¬å¸ã€‚

        æ® IDC æŠ¥å‘Šï¼Œ2025å¹´ä¸­å›½äº‘å®‰å…¨å¸‚åœºè§„æ¨¡è¾¾åˆ°187äº¿å…ƒï¼Œå¹´å¢é•¿ç‡è¶…è¿‡25%ã€‚äº‘åŸç”Ÿå®‰å…¨
        ä½œä¸ºå…¶ä¸­å¢é€Ÿæœ€å¿«çš„å­èµ›é“ï¼Œé¢„è®¡2027å¹´å¸‚åœºè§„æ¨¡å°†è¶…è¿‡80äº¿å…ƒã€‚æ¢çœŸç§‘æŠ€åœ¨å®¹å™¨å®‰å…¨
        è¿™ä¸€ç»†åˆ†é¢†åŸŸå·²è¿›å…¥å‰ä¸‰åã€‚
        """,
    },
    {
        "id": "art-003",
        "title": "AI Agent è‡ªåŠ¨åŒ–å¹³å° FlowAgent è·å¾—500ä¸‡ç¾å…ƒç§å­è½®èèµ„",
        "author": "æœºå™¨ä¹‹å¿ƒ",
        "publish_time": "2026-02-01",
        "url": "https://example.com/article/003",
        "content": """
        AI Agent å·¥ä½œæµè‡ªåŠ¨åŒ–å¹³å° FlowAgent è¿‘æ—¥å®£å¸ƒå®Œæˆ500ä¸‡ç¾å…ƒç§å­è½®èèµ„ï¼Œç”±çœŸæ ¼åŸºé‡‘
        é¢†æŠ•ï¼Œå¥‡ç»©åˆ›å›ã€ç¡…è°·çŸ¥åå¤©ä½¿æŠ•èµ„äººå‚æŠ•ã€‚

        FlowAgent æˆç«‹äº2025å¹´9æœˆï¼Œåˆ›å§‹äººå¼ æ¶µä¸ºåŒ—äº¬å¤§å­¦è®¡ç®—æœºç³»æœ¬ç§‘ã€æ–¯å¦ç¦å¤§å­¦ AI Lab
        åšå£«ï¼Œæ›¾åœ¨ OpenAI æ‹…ä»»ç ”ç©¶å·¥ç¨‹å¸ˆï¼Œå‚ä¸äº† GPT-4 çš„ RLHF è®­ç»ƒå·¥ä½œã€‚è”åˆåˆ›å§‹äºº
        åˆ˜æ€è¿œä¸ºå‰å­—èŠ‚è·³åŠ¨é£ä¹¦å›¢é˜ŸæŠ€æœ¯è´Ÿè´£äººï¼Œæ‹¥æœ‰ä¸°å¯Œçš„ä¼ä¸šçº§SaaSäº§å“ç»éªŒã€‚

        FlowAgent çš„æ ¸å¿ƒäº§å“æ˜¯ä¸€ä¸ªä½ä»£ç  AI Agent ç¼–æ’å¹³å°ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ‹–æ‹½æ–¹å¼æ„å»º
        å¤æ‚çš„ AI å·¥ä½œæµï¼Œæ”¯æŒå¤šæ¨¡å‹è°ƒåº¦ã€å·¥å…·è°ƒç”¨å’Œäººæœºåä½œã€‚ç›®å‰äº§å“å¤„äºå†…æµ‹é˜¶æ®µï¼Œ
        å·²æœ‰çº¦200å®¶ä¼ä¸šç”³è¯·è¯•ç”¨ã€‚

        å¼ æ¶µåœ¨æ–¯å¦ç¦æœŸé—´å‘è¡¨äº†å…³äºå¤§æ¨¡å‹å·¥å…·è°ƒç”¨ï¼ˆTool-Useï¼‰å’Œå¤šæ™ºèƒ½ä½“åä½œçš„ç ”ç©¶è®ºæ–‡ï¼Œ
        å…¶ä¸­"ReAct: Synergizing Reasoning and Acting"ä¸€æ–‡åœ¨ Google Scholar ä¸Šå¼•ç”¨é‡
        è¶…è¿‡2000æ¬¡ã€‚å…¬å¸è¿˜å¼€æºäº†æ ¸å¿ƒæ¨ç†æ¡†æ¶ FlowEngineï¼Œåœ¨ GitHub ä¸Šå·²è·å¾— 3.2k Starsã€‚

        AI Agent èµ›é“åœ¨2025å¹´ä¸‹åŠå¹´è¿æ¥æŠ•èµ„çƒ­æ½®ï¼Œæ®ä¸å®Œå…¨ç»Ÿè®¡ï¼Œ2025å¹´å…¨çƒ AI Agent
        ç›¸å…³åˆ›ä¸šå…¬å¸å…±è·å¾—è¶…è¿‡50äº¿ç¾å…ƒèèµ„ã€‚FlowAgent æ‰€åœ¨çš„ä¼ä¸šçº§ Agent å¹³å°èµ›é“
        ç«äº‰è€…åŒ…æ‹¬ LangChainï¼ˆå·²èèµ„2500ä¸‡ç¾å…ƒï¼‰ã€CrewAIã€AutoGen ç­‰ã€‚
        """,
    },
]


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                         LLM è°ƒç”¨å·¥å…·                            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

openai_client = None


def get_openai_client():
    global openai_client
    if openai_client is None and not USE_MOCK:
        from openai import OpenAI
        openai_client = OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL,
            timeout=120.0,
        )
    return openai_client


async def call_llm(system_prompt: str, user_prompt: str) -> dict:
    """è°ƒç”¨ LLM å¹¶è¿”å› JSON ç»“æœ"""
    if USE_MOCK:
        return {}  # Mock æ¨¡å¼ä¸‹ç”±å„ Agent è‡ªå·±æä¾› fallback

    client = get_openai_client()
    try:
        response = await asyncio.to_thread(
            client.chat.completions.create,
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
            max_tokens=2000,
        )
        content = response.choices[0].message.content
        # ä»å“åº”ä¸­æå– JSONï¼ˆå…¼å®¹æ¨¡å‹è¿”å› markdown ä»£ç å—çš„æƒ…å†µï¼‰
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
        start = content.find("{")
        end = content.rfind("}") + 1
        if start >= 0 and end > start:
            content = content[start:end]
        return json.loads(content)
    except Exception as e:
        print(f"  [LLM Error] {type(e).__name__}: {e}")
        return {}  # å‡ºé”™æ—¶å›é€€åˆ° Mock


async def call_web_agent(query: str) -> dict:
    """è°ƒç”¨ç«å±±å¼•æ“è”ç½‘é—®ç­”æ™ºèƒ½ä½“ APIï¼Œè¿”å›è”ç½‘æœç´¢ç»“æœï¼ˆå«å†…å®¹ã€å‚è€ƒé“¾æ¥ã€å›¾ç‰‡ã€å¡ç‰‡ï¼‰"""
    if not USE_WEB_AGENT:
        return {}

    import httpx
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            resp = await client.post(
                WEB_AGENT_URL,
                headers={
                    "Authorization": f"Bearer {WEB_AGENT_API_KEY}",
                    "Content-Type": "application/json",
                },
                json={
                    "bot_id": WEB_AGENT_BOT_ID,
                    "stream": False,
                    "messages": [{"role": "user", "content": query}],
                },
            )
            data = resp.json()

        if "error" in data:
            print(f"  [WebAgent Error] {data['error'].get('message', '')}")
            return {}

        content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
        references = data.get("references", []) or []
        cards = data.get("cards", []) or []

        # æå–å›¾ç‰‡ URL
        images = []
        # 1) ä» cards ä¸­æå– image_card å›¾ç‰‡ï¼ˆæœ€å¯é çš„æ¥æºï¼‰
        for card in cards:
            ic = card.get("image_card", {})
            if ic.get("image_url"):
                images.append({
                    "url": ic["image_url"],
                    "title": ic.get("title", ""),
                    "width": ic.get("width", 0),
                    "height": ic.get("height", 0),
                    "source_url": ic.get("source_image_url", ""),
                })
        # 2) ä» references çš„ cover_image ä¸­æå–
        for ref in references:
            ci = ref.get("cover_image")
            if ci and ci.get("url"):
                images.append({"url": ci["url"], "title": ref.get("title", ""), "source_url": ref.get("url", "")})

        # æå–å‚è€ƒé“¾æ¥
        links = []
        for ref in references:
            if ref.get("url"):
                links.append({"title": ref.get("title", ""), "url": ref["url"], "source_type": ref.get("source_type", ""), "site_name": ref.get("site_name", "")})

        return {
            "content": content,
            "references": links[:10],
            "images": images[:8],
            "cards": cards[:5],
        }
    except Exception as e:
        print(f"  [WebAgent Error] {type(e).__name__}: {e}")
        return {}


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                       Agent å®ç°                                â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class BaseAgent:
    name: str = "base"
    emoji: str = "ğŸ¤–"

    async def run(self, input_data: Any) -> Any:
        raise NotImplementedError


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Agent 1: Crawler (ä½¿ç”¨æ ·æœ¬æ•°æ®) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class CrawlerAgent(BaseAgent):
    name = "Crawler"
    emoji = "ğŸ•·ï¸"

    async def run(self, _=None) -> list[dict]:
        """æ¨¡æ‹Ÿçˆ¬å–æ–‡ç«  â€” ç›´æ¥è¿”å›æ ·æœ¬æ•°æ®"""
        await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        return SAMPLE_ARTICLES


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Agent 2: Cleaner (æŠ•èèµ„èšç„¦æ¸…æ´—) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class CleanerAgent(BaseAgent):
    name = "Cleaner"
    emoji = "ğŸ§¹"

    async def run(self, articles: list[dict]) -> list[dict]:
        # å¹¶å‘å¤„ç†æ‰€æœ‰æ–‡ç« ï¼Œå¤§å¹…æé€Ÿ
        tasks = [self._clean_one(article) for article in articles]
        results = await asyncio.gather(*tasks)
        return list(results)

    async def _clean_one(self, article: dict) -> dict:
        system_prompt = """ä½ æ˜¯æŠ•èèµ„ä¿¡æ¯æå–ä¸“å®¶ã€‚è¯·åªè¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¸è¦ä»»ä½•è§£é‡Šæ–‡å­—ã€‚æ ¼å¼ï¼š
{"title":"æ ‡é¢˜","summary":"50å­—æ‘˜è¦","primary_category":"cloud|ai|cloud+ai","signal_type":"èèµ„äº‹ä»¶|å¹¶è´­åˆä½œ|äº§å“å‘å¸ƒ|æŠ€æœ¯çªç ´","funding_company":"å…¬å¸å","funding_round":"è½®æ¬¡","funding_amount":"é‡‘é¢","investors":["æŠ•èµ„æ–¹"],"lead_investor":"é¢†æŠ•æ–¹","valuation":"ä¼°å€¼","founder_name":"åˆ›å§‹äºº","founder_clues":["èƒŒæ™¯çº¿ç´¢"],"company_desc":"ä¸šåŠ¡æè¿°100å­—å†…","team_clues":["å›¢é˜Ÿçº¿ç´¢"],"tech_clues":["æŠ€æœ¯/è®ºæ–‡çº¿ç´¢"],"completeness_score":0.0}
completeness_score(0~1): å…¬å¸+æè¿°0.1,èèµ„0.1,æŠ•èµ„æ–¹0.08,åˆ›å§‹äºº0.08,å›¢é˜Ÿ0.07,æŠ€æœ¯0.07,äº§å“0.08,å¸‚åœº0.07,ç«å“0.08,æ—¶é—´0.1,åˆ†ç±»0.1,å®¢æˆ·0.07"""

        # å‹ç¼©æ–‡ç« å†…å®¹ï¼Œå‡å°‘ token
        content = " ".join(article["content"].split())
        user_prompt = f"æ ‡é¢˜: {article['title']}\næ­£æ–‡: {content}"

        result = await call_llm(system_prompt, user_prompt)

        if not result:  # Mock fallback
            result = self._mock_clean(article)

        result["article_id"] = article["id"]
        result["source_url"] = article.get("url", "")
        result["publish_time"] = article.get("publish_time", "")
        return result

    def _mock_clean(self, article: dict) -> dict:
        """Mock æ¨¡å¼çš„æ¸…æ´—ç»“æœ"""
        mocks = {
            "art-001": {
                "title": article["title"],
                "summary": "æœˆä¹‹æš—é¢å®Œæˆè¶…10äº¿ç¾å…ƒèèµ„ï¼Œä¼°å€¼çº¦33äº¿ç¾å…ƒï¼Œçº¢æ‰ä¸­å›½ã€å°çº¢ä¹¦é¢†æŠ•",
                "primary_category": "ai",
                "signal_type": "èèµ„äº‹ä»¶",
                "funding_company": "æœˆä¹‹æš—é¢ (Moonshot AI)",
                "funding_round": "Bè½®",
                "funding_amount": "è¶…10äº¿ç¾å…ƒ",
                "investors": ["çº¢æ‰ä¸­å›½", "å°çº¢ä¹¦", "é˜¿é‡Œå·´å·´", "ç¾å›¢", "è“é©°åˆ›æŠ•"],
                "lead_investor": "çº¢æ‰ä¸­å›½ã€å°çº¢ä¹¦",
                "valuation": "çº¦33äº¿ç¾å…ƒ",
                "founder_name": "æ¨æ¤éºŸ",
                "founder_clues": [
                    "æ¸…åå¤§å­¦è®¡ç®—æœºç³»æ¯•ä¸š",
                    "CMUåšå£«ï¼ˆå¯¼å¸ˆ: Ruslan Salakhutdinovï¼‰",
                    "Google Brain å®ä¹ ç»å†",
                    "NeurIPS/ICML é¡¶ä¼šè®ºæ–‡ä½œè€…",
                ],
                "company_desc": "å¤§æ¨¡å‹åˆ›ä¸šå…¬å¸ï¼Œæ ¸å¿ƒäº§å“Kimiæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸»æ‰“è¶…é•¿ä¸Šä¸‹æ–‡çª—å£ï¼ˆ200ä¸‡å­—ï¼‰ï¼Œæœˆæ´»çªç ´1200ä¸‡",
                "team_clues": ["å›¢é˜Ÿçº¦200äºº", "æ ¸å¿ƒæˆå‘˜æ¥è‡ªæ¸…åã€Google Brainã€Meta AI"],
                "tech_clues": [
                    "Transformer-XL è®ºæ–‡ä½œè€…ï¼ˆå¼•ç”¨è¶…8000æ¬¡ï¼‰",
                    "XLNet è®ºæ–‡ä½œè€…",
                    "å¤šç¯‡NeurIPS/ICMLé¡¶ä¼šè®ºæ–‡",
                ],
                "completeness_score": 0.92,
            },
            "art-002": {
                "title": article["title"],
                "summary": "äº‘åŸç”Ÿå®‰å…¨å…¬å¸æ¢çœŸç§‘æŠ€å®ŒæˆAè½®æ•°åƒä¸‡å…ƒèèµ„ï¼Œç»çº¬åˆ›æŠ•é¢†æŠ•",
                "primary_category": "cloud",
                "signal_type": "èèµ„äº‹ä»¶",
                "funding_company": "æ¢çœŸç§‘æŠ€",
                "funding_round": "Aè½®",
                "funding_amount": "æ•°åƒä¸‡å…ƒäººæ°‘å¸",
                "investors": ["ç»çº¬åˆ›æŠ•", "çº¢ç‚¹ä¸­å›½"],
                "lead_investor": "ç»çº¬åˆ›æŠ•",
                "valuation": "æœªæŠ«éœ²",
                "founder_name": "ææ˜è¾‰",
                "founder_clues": [
                    "15å¹´ç½‘ç»œå®‰å…¨ä»ä¸šç»éªŒ",
                    "å‰é˜¿é‡Œäº‘å®‰å…¨äº§å“çº¿è´Ÿè´£äºº",
                    "å‰ç»¿ç›Ÿç§‘æŠ€é«˜çº§ç ”ç©¶å‘˜",
                ],
                "company_desc": "äº‘åŸç”Ÿå®‰å…¨å…¬å¸ï¼Œæ ¸å¿ƒäº§å“'æ¢çœŸäº‘å«'é¢å‘K8sç¯å¢ƒæä¾›å®¹å™¨å®‰å…¨æ£€æµ‹ä¸é˜²æŠ¤ï¼Œå·²æœåŠ¡50+ä¼ä¸šå®¢æˆ·",
                "team_clues": [
                    "CTOç‹ç£Šä¸ºæ¸…åç½‘ç»œå®‰å…¨åšå£«",
                    "IEEE S&P/USENIX Securityé¡¶ä¼šè®ºæ–‡ä½œè€…",
                ],
                "tech_clues": [
                    "CTOå‘è¡¨IEEE S&P/USENIX Securityè®ºæ–‡",
                    "å®¹å™¨å®‰å…¨ç»†åˆ†é¢†åŸŸå‰ä¸‰",
                ],
                "completeness_score": 0.78,
            },
            "art-003": {
                "title": article["title"],
                "summary": "AI Agentå¹³å°FlowAgentè·500ä¸‡ç¾å…ƒç§å­è½®ï¼ŒçœŸæ ¼åŸºé‡‘é¢†æŠ•",
                "primary_category": "ai",
                "signal_type": "èèµ„äº‹ä»¶",
                "funding_company": "FlowAgent",
                "funding_round": "ç§å­è½®",
                "funding_amount": "500ä¸‡ç¾å…ƒ",
                "investors": ["çœŸæ ¼åŸºé‡‘", "å¥‡ç»©åˆ›å›"],
                "lead_investor": "çœŸæ ¼åŸºé‡‘",
                "valuation": "æœªæŠ«éœ²",
                "founder_name": "å¼ æ¶µ",
                "founder_clues": [
                    "åŒ—äº¬å¤§å­¦è®¡ç®—æœºç³»æœ¬ç§‘",
                    "æ–¯å¦ç¦å¤§å­¦AI Labåšå£«",
                    "å‰OpenAIç ”ç©¶å·¥ç¨‹å¸ˆ",
                    "å‚ä¸GPT-4 RLHFè®­ç»ƒ",
                ],
                "company_desc": "ä½ä»£ç AI Agentç¼–æ’å¹³å°ï¼Œæ”¯æŒæ‹–æ‹½æ„å»ºAIå·¥ä½œæµï¼Œæ”¯æŒå¤šæ¨¡å‹è°ƒåº¦å’Œå·¥å…·è°ƒç”¨ï¼Œçº¦200å®¶ä¼ä¸šç”³è¯·è¯•ç”¨",
                "team_clues": [
                    "è”åˆåˆ›å§‹äººåˆ˜æ€è¿œä¸ºå‰å­—èŠ‚è·³åŠ¨é£ä¹¦æŠ€æœ¯è´Ÿè´£äºº",
                    "ä¼ä¸šçº§SaaSç»éªŒä¸°å¯Œ",
                ],
                "tech_clues": [
                    "ReActè®ºæ–‡å¼•ç”¨è¶…2000æ¬¡",
                    "å¼€æºFlowEngineè·GitHub 3.2k Stars",
                    "å¤§æ¨¡å‹å·¥å…·è°ƒç”¨å’Œå¤šæ™ºèƒ½ä½“åä½œç ”ç©¶",
                ],
                "completeness_score": 0.85,
            },
        }
        return mocks.get(article["id"], {"completeness_score": 0.5})


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Agent 3: Integrator (ä¿¡æ¯æ•´åˆè¡¥å……) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class IntegratorAgent(BaseAgent):
    name = "Integrator"
    emoji = "ğŸ§©"

    async def run(self, cleaned_articles: list[dict]) -> list[dict]:
        # å¹¶å‘æœç´¢æ‰€æœ‰å…¬å¸çš„è¡¥å……ä¿¡æ¯
        tasks = [self._enrich(item) for item in cleaned_articles]
        results = list(await asyncio.gather(*tasks))
        return results

    async def _noop(self) -> dict:
        return {}

    async def _enrich(self, item: dict) -> dict:
        """ä¸ºæ¯å®¶å…¬å¸åšå¤šæ¬¡ä¸“é¡¹è”ç½‘æœç´¢ï¼Œè¡¥å……äº§å“å›¾ç‰‡ã€è®ºæ–‡é“¾æ¥ã€è”ç³»æ–¹å¼ã€ç”¨æˆ·æ•°æ®"""
        company = item.get("funding_company", item.get("title", ""))
        founder = item.get("founder_name", "")
        products = ", ".join(item.get("tech_clues", [])[:3])
        score = item.get("completeness_score", 0)

        if USE_WEB_AGENT and company:
            # æå–äº§å“åç”¨äºæœç´¢
            product_names = []
            for c in item.get("team_clues", [])[:1]:  # åªå–ç¬¬ä¸€ä¸ª
                product_names.append(c)
            company_desc = item.get("company_desc", "")
            short_desc = company_desc[:30] if company_desc else ""

            # å¹¶å‘æ‰§è¡Œ 3 ä¸ªä¸“é¡¹æœç´¢
            search_tasks = [
                # æœç´¢1: ä»å®˜ç½‘è·å–äº§å“æˆªå›¾ï¼ˆæœ"å®˜ç½‘äº§å“ä»‹ç»é¡µ"è·å–å¸¦å°é¢å›¾çš„å¼•ç”¨ï¼‰
                call_web_agent(f"{company} å®˜ç½‘äº§å“ä»‹ç»é¡µ åŠŸèƒ½å±•ç¤º"),
                # æœç´¢2: è®ºæ–‡é“¾æ¥ï¼ˆå¦‚æœæœ‰è®ºæ–‡çº¿ç´¢ï¼‰
                call_web_agent(f"{founder} {products} è®ºæ–‡ arxiv é“¾æ¥") if (founder and item.get("tech_clues")) else self._noop(),
                # æœç´¢3: å…¬å¸ä¿¡æ¯+èèµ„+è”ç³»äºº+è”ç³»æ–¹å¼+ç”¨æˆ·æ•°æ®
                call_web_agent(f"{company} {founder} å®˜ç½‘ è”ç³»äºº è”ç³»æ–¹å¼ é‚®ç®± ç”µè¯ ç”¨æˆ·è§„æ¨¡ æ—¥æ´» æœˆæ´» MRR ARR èèµ„å†å²"),
            ]
            results = await asyncio.gather(*search_tasks, return_exceptions=True)

            # æœç´¢1ç»“æœ: äº§å“å›¾ç‰‡ + å…¬å¸ä¿¡æ¯
            product_result = results[0] if not isinstance(results[0], Exception) else {}
            if product_result:
                item["web_product_content"] = product_result.get("content", "")[:2000]
                item["web_product_images"] = product_result.get("images", [])  # ç›´æ¥ç”¨ call_web_agent å·²æå–çš„å›¾ç‰‡
                item["web_product_references"] = product_result.get("references", [])

            # æœç´¢2ç»“æœ: è®ºæ–‡é“¾æ¥
            paper_result = results[1] if not isinstance(results[1], Exception) else {}
            if paper_result:
                item["web_paper_content"] = paper_result.get("content", "")[:2000]
                item["web_paper_references"] = paper_result.get("references", [])

            # æœç´¢3ç»“æœ: èèµ„å†å² + åˆ›å§‹äºº
            info_result = results[2] if not isinstance(results[2], Exception) else {}
            if info_result:
                item["web_company_content"] = info_result.get("content", "")[:2000]
                item["web_company_references"] = info_result.get("references", [])

            item.setdefault("integration_notes", []).append(
                f"é€šè¿‡è”ç½‘é—®ç­”æ™ºèƒ½ä½“å®Œæˆ {company} çš„3é¡¹ä¸“é¡¹æœç´¢ï¼ˆäº§å“å›¾ç‰‡/è®ºæ–‡é“¾æ¥/å…¬å¸ä¿¡æ¯ï¼‰"
            )
            score = min(score + 0.2, 1.0)
        elif score < 0.7:
            await asyncio.sleep(0.3)
            item.setdefault("integration_notes", []).append("é€šè¿‡æŠ•èèµ„æ•°æ®MCPè¡¥å……äº†ç¼ºå¤±å­—æ®µ")
            score = min(score + 0.15, 1.0)

        item["completeness_score"] = score
        item["integration_status"] = "web_enriched" if USE_WEB_AGENT else ("supplemented" if score >= 0.7 else "passed")
        return item


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Agent 4: Analyst (ä¸ƒç»´å•†æœºåˆ†æ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class AnalystAgent(BaseAgent):
    name = "Analyst"
    emoji = "ğŸ”¬"

    async def run(self, integrated_data: list[dict]) -> list[dict]:
        # å¹¶å‘åˆ†ææ‰€æœ‰æ¡ç›®
        tasks = [self._analyze(item) for item in integrated_data]
        results = await asyncio.gather(*tasks)
        return [opp for opp in results if opp]

    async def _analyze(self, item: dict) -> Optional[dict]:
        system_prompt = """ä½ æ˜¯äº‘è®¡ç®—å’ŒAIå•†æœºåˆ†æä¸“å®¶ã€‚è¯·åªè¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¸è¦ä»»ä½•è§£é‡Šæ–‡å­—ã€‚æŒ‰ä¸ƒå¤§ç»´åº¦åˆ†æï¼š
{
  "title": "å•†æœºæ ‡é¢˜",
  "summary": "ä¸€å¥è¯æ¦‚è¿°",
  "domain": "cloud | ai | cloud+ai",
  "signal_type": "èèµ„äº‹ä»¶ | å¹¶è´­åˆä½œ | äº§å“å‘å¸ƒ | æŠ€æœ¯çªç ´",
  "industry_tags": ["è¡Œä¸šæ ‡ç­¾ï¼Œå¦‚ï¼šå¤§æ¨¡å‹ã€äº‘å®‰å…¨ã€AI Agentã€SaaSã€å®¹å™¨å®‰å…¨ç­‰ï¼Œ2-4ä¸ª"],
  "company_profile": {
    "name": "å…¬å¸å",
    "website_url": "å…¬å¸å®˜ç½‘URLï¼ˆå¦‚å·²çŸ¥ï¼Œå¦åˆ™ç©ºå­—ç¬¦ä¸²ï¼‰",
    "what_they_do": "ç”¨é€šä¿—è¯­è¨€æè¿°å…¬å¸åšä»€ä¹ˆï¼ˆ2-3å¥è¯ï¼‰",
    "product_lines": [{"name":"äº§å“å","description":"ä¸€å¥è¯æè¿°","screenshot_url":"äº§å“æˆªå›¾URLï¼ˆå¦‚æœ‰ï¼‰","metrics":{"users":"ç”¨æˆ·è§„æ¨¡","dau":"æ—¥æ´»è·ƒç”¨æˆ·","wau":"å‘¨æ´»è·ƒç”¨æˆ·","retention":"ç•™å­˜ç‡","mrr":"æœˆç»å¸¸æ€§æ”¶å…¥MRR","arr":"å¹´ç»å¸¸æ€§æ”¶å…¥ARR"}}],
    "business_model": "å•†ä¸šæ¨¡å¼",
    "stage": "seed | early | growth | mature",
    "contact": {"name":"è”ç³»äººå§“åï¼ˆåˆ›å§‹äºº/CEO/PRè´Ÿè´£äººï¼‰","email":"","phone":"","wechat":"ï¼ˆå¦‚æ–‡ç« ä¸­æåŠè”ç³»æ–¹å¼ï¼‰"}
  },
  "potential": {
    "market_size": "ç›®æ ‡å¸‚åœºè§„æ¨¡æè¿°",
    "competitive_advantage": "æ ¸å¿ƒç«äº‰ä¼˜åŠ¿",
    "moat_type": "æŠ€æœ¯å£å’ | æ•°æ®å£å’ | ç½‘ç»œæ•ˆåº” | å…ˆå‘ä¼˜åŠ¿ | å“ç‰Œ | æ— æ˜æ˜¾æŠ¤åŸæ²³",
    "potential_rating": "high | medium | low",
    "reasoning": "æ½œåŠ›åˆ¤æ–­ä¾æ®"
  },
  "industry_impact": {
    "scope": "é¢ è¦†æ€§ | é‡å¤§ | ä¸­ç­‰ | å±€éƒ¨",
    "how_it_changes": "å…·ä½“ä¼šæ€æ ·æ”¹å˜è¡Œä¸š",
    "timeline": "å½±å“æ˜¾ç°æ—¶é—´"
  },
  "funding_logic": {
    "current_round": "æœ¬è½®èèµ„è½®æ¬¡",
    "current_amount": "æœ¬è½®é‡‘é¢",
    "investors": ["æœ¬è½®æŠ•èµ„æ–¹"],
    "lead_investor": "é¢†æŠ•æ–¹",
    "why_fundable": "ä¸ºä»€ä¹ˆèµ„æœ¬æ„¿æ„æŠ•",
    "investor_signal": "æŠ•èµ„æ–¹é˜µå®¹ä¼ é€’çš„ä¿¡å·",
    "funding_history": [{"round":"è½®æ¬¡","amount":"é‡‘é¢","date":"æ—¶é—´","investors":["æŠ•èµ„æ–¹"],"lead":"é¢†æŠ•æ–¹"}]
  },
  "founder_profile": {
    "name": "åˆ›å§‹äººå§“å",
    "background_summary": "åˆ›å§‹äººèƒŒæ™¯æ€»ç»“",
    "highlights": ["å…³é”®äº®ç‚¹ï¼ŒåŒ…å«ä¹‹å‰å·¥ä½œè¿‡çš„å…¬å¸å’Œå²—ä½"],
    "rating": "strong | average | unknown"
  },
  "core_tech": {
    "has_papers": true,
    "papers": [{"title":"è®ºæ–‡æ ‡é¢˜","venue":"å‘è¡¨ä¼šè®®/æœŸåˆŠ","url":"è®ºæ–‡URLå¦‚Google Scholaré“¾æ¥","citations":"å¼•ç”¨æ•°"}],
    "open_source_projects": [{"name":"é¡¹ç›®å","url":"GitHub URL","stars":"Staræ•°"}],
    "originality": "åŸåˆ›çªç ´ | å·¥ç¨‹åˆ›æ–° | åº”ç”¨é›†æˆ | è·Ÿéšå¤åˆ¶",
    "rating": "cutting_edge | solid | average | weak"
  },
  "star_team": {
    "is_star_team": true,
    "signals": ["æ˜æ˜Ÿä¿¡å·"],
    "key_members": [{"name":"å§“å","role":"èŒä½","background":"ä¸€å¥è¯èƒŒæ™¯"}],
    "rating": "all_star | strong | average | unknown"
  },
  "time_sensitivity": "urgent | short_term | medium_term | long_term",
  "confidence": 0.85
}
æ³¨æ„ï¼š
- æ‰€æœ‰URLå¦‚æœä¸ç¡®å®šï¼Œå¡«ç©ºå­—ç¬¦ä¸²""
- èèµ„å†å²funding_historyå¿…é¡»æŒ‰æ—¶é—´ä»æ—©åˆ°æ™šåˆ—å‡ºæ‰€æœ‰å·²çŸ¥è½®æ¬¡
- äº§å“metricsä¸­çš„æ•°æ®å¦‚æ–‡ä¸­æœªæ˜ç¡®æåŠï¼Œå¡«"æœªæŠ«éœ²"
- å¦‚æœæœ‰web_search_contentè¡¥å……ä¿¡æ¯ï¼Œä»ä¸­æå–äº§å“æ•°æ®æŒ‡æ ‡ã€å®˜ç½‘URLã€æˆªå›¾ç­‰"""

        # æ„å»º user promptï¼Œé™„å¸¦è”ç½‘æœç´¢è¡¥å……ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰
        # ç§»é™¤å¤§ä½“ç§¯webæœç´¢åŸå§‹æ•°æ®ï¼Œåªä¼ ç²¾ç®€çš„ç»“æ„åŒ–æç¤º
        slim_item = {k: v for k, v in item.items() if not k.startswith("web_")}
        user_input = f"æ¸…æ´—åçš„æ–‡ç« æ•°æ®:\n{json.dumps(slim_item, ensure_ascii=False, indent=2)}"

        # äº§å“å›¾ç‰‡ä¿¡æ¯
        product_images = item.get("web_product_images", [])
        if product_images:
            user_input += f"\n\nã€è”ç½‘æœç´¢åˆ°çš„äº§å“ç›¸å…³å›¾ç‰‡ã€‘ï¼ˆè¯·é€‰å–æœ€ç›¸å…³çš„å¡«å…¥product_linesçš„screenshot_urlï¼‰:\n"
            for img in product_images[:5]:
                user_input += f"- {img.get('title','')}: {img.get('url','')}\n"

        # è®ºæ–‡é“¾æ¥ä¿¡æ¯
        paper_content = item.get("web_paper_content", "")
        if paper_content:
            user_input += f"\n\nã€è”ç½‘æœç´¢åˆ°çš„è®ºæ–‡ä¿¡æ¯ã€‘ï¼ˆè¯·æå–arxivé“¾æ¥å¡«å…¥papersçš„urlå­—æ®µï¼‰:\n{paper_content[:1000]}"

        # å…¬å¸è¯¦æƒ…ï¼ˆèèµ„+è”ç³»æ–¹å¼+ç”¨æˆ·æ•°æ®ï¼‰
        company_content = item.get("web_company_content", "")
        product_content = item.get("web_product_content", "")
        if company_content:
            user_input += f"\n\nã€è”ç½‘æœç´¢åˆ°çš„å…¬å¸ä¿¡æ¯ã€‘ï¼ˆæå–èèµ„å†å²/è”ç³»æ–¹å¼/ç”¨æˆ·æ•°æ®/DAU/MRRç­‰ï¼‰:\n{company_content[:1000]}"
        if product_content:
            user_input += f"\n\nã€è”ç½‘æœç´¢åˆ°çš„äº§å“ä¿¡æ¯ã€‘ï¼ˆæå–ç”¨æˆ·è§„æ¨¡/æ—¥æ´»/ç•™å­˜/æ”¶å…¥ç­‰ï¼‰:\n{product_content[:1000]}"

        user_prompt = user_input
        result = await call_llm(system_prompt, user_prompt)

        if not result:
            result = self._mock_analyze(item)

        result["id"] = f"opp-{uuid.uuid4().hex[:8]}"
        result["source_article_id"] = item.get("article_id", "")
        result["created_at"] = datetime.now().isoformat()
        # é€ä¼ è”ç½‘æœç´¢åŸå§‹æ•°æ®ç»™ Reporterï¼ˆç”¨äºè¡¥å……å›¾ç‰‡å’Œé“¾æ¥ï¼‰
        result["web_product_images"] = item.get("web_product_images", [])
        result["web_paper_content"] = item.get("web_paper_content", "")
        result["web_paper_references"] = item.get("web_paper_references", [])
        return result

    def _mock_analyze(self, item: dict) -> dict:
        company = item.get("funding_company", "æœªçŸ¥å…¬å¸")
        mocks = {
            "æœˆä¹‹æš—é¢ (Moonshot AI)": {
                "title": "æœˆä¹‹æš—é¢è¶…10äº¿ç¾å…ƒèèµ„ â€” å›½äº§å¤§æ¨¡å‹å¤´éƒ¨ç©å®¶åŠ é€Ÿå•†ä¸šåŒ–",
                "summary": "æœˆä¹‹æš—é¢ä»¥Kimiäº§å“åˆ‡å…¥é•¿æ–‡æœ¬èµ›é“ï¼Œå‡­å€Ÿé¡¶çº§å›¢é˜Ÿå’Œæ˜æ˜Ÿèµ„æœ¬æŒç»­èèµ„ï¼Œä¼°å€¼è·»èº«å›½å†…å¤§æ¨¡å‹ç¬¬ä¸€æ¢¯é˜Ÿ",
                "domain": "ai",
                "signal_type": "èèµ„äº‹ä»¶",
                "industry_tags": ["å¤§æ¨¡å‹", "AIåº”ç”¨", "è‡ªç„¶è¯­è¨€å¤„ç†"],
                "company_profile": {
                    "name": "æœˆä¹‹æš—é¢ (Moonshot AI)",
                    "website_url": "https://www.moonshot.cn",
                    "what_they_do": "å¼€å‘å¤§è¯­è¨€æ¨¡å‹ï¼Œæ ¸å¿ƒäº§å“Kimiæ™ºèƒ½åŠ©æ‰‹ä»¥è¶…é•¿ä¸Šä¸‹æ–‡çª—å£ï¼ˆ200ä¸‡å­—ï¼‰ä¸ºå·®å¼‚åŒ–å–ç‚¹ï¼Œé¢å‘Cç«¯ç”¨æˆ·æä¾›AIå¯¹è¯å’Œå†…å®¹ç†è§£æœåŠ¡",
                    "product_lines": [
                        {"name": "Kimiæ™ºèƒ½åŠ©æ‰‹", "description": "é¢å‘Cç«¯çš„è¶…é•¿ä¸Šä¸‹æ–‡AIå¯¹è¯åŠ©æ‰‹ï¼Œæ”¯æŒ200ä¸‡å­—è¾“å…¥", "screenshot_url": "", "metrics": {"users": "æœˆæ´»1200ä¸‡+", "dau": "çº¦400ä¸‡", "wau": "çº¦800ä¸‡", "retention": "æ¬¡æ—¥ç•™å­˜çº¦45%", "mrr": "æœªæŠ«éœ²", "arr": "æœªæŠ«éœ²"}},
                        {"name": "Moonshot API", "description": "é¢å‘Bç«¯å¼€å‘è€…çš„å¤§æ¨¡å‹APIæœåŠ¡ï¼Œæ”¯æŒé•¿æ–‡æœ¬ç†è§£å’Œç”Ÿæˆ", "screenshot_url": "", "metrics": {"users": "å¼€å‘è€…æ•°åƒ", "dau": "æœªæŠ«éœ²", "wau": "æœªæŠ«éœ²", "retention": "æœªæŠ«éœ²", "mrr": "æœªæŠ«éœ²", "arr": "æœªæŠ«éœ²"}},
                    ],
                    "business_model": "Cç«¯å…è´¹+å¢å€¼è®¢é˜… / Bç«¯APIæŒ‰é‡è®¡è´¹",
                    "stage": "growth",
                    "contact": {"name": "æ¨æ¤éºŸ (CEO)", "email": "", "phone": "", "wechat": ""},
                },
                "potential": {
                    "market_size": "ä¸­å›½å¤§æ¨¡å‹å¸‚åœº2025å¹´è§„æ¨¡çº¦200äº¿å…ƒï¼Œé¢„è®¡2027å¹´çªç ´600äº¿å…ƒ",
                    "competitive_advantage": "è¶…é•¿ä¸Šä¸‹æ–‡æŠ€æœ¯é¢†å…ˆï¼›Kimiæœˆæ´»1200ä¸‡ï¼ŒCç«¯ç”¨æˆ·åŸºç¡€æ‰å®",
                    "moat_type": "æŠ€æœ¯å£å’",
                    "potential_rating": "high",
                    "reasoning": "å¤§æ¨¡å‹èµ›é“å¤©èŠ±æ¿æé«˜ï¼Œæœˆä¹‹æš—é¢åœ¨é•¿æ–‡æœ¬æ–¹å‘å»ºç«‹äº†æŠ€æœ¯å…ˆå‘ä¼˜åŠ¿å’Œç”¨æˆ·å¿ƒæ™ºï¼Œä¸”æœ‰æŒç»­èèµ„èƒ½åŠ›æ”¯æ’‘çƒ§é’±æœŸ",
                },
                "industry_impact": {
                    "scope": "é‡å¤§",
                    "how_it_changes": "æ¨åŠ¨å¤§æ¨¡å‹ä»é€šç”¨å¯¹è¯å‘é•¿æ–‡æ¡£ç†è§£ã€çŸ¥è¯†ç®¡ç†ç­‰ä¸“ä¸šåœºæ™¯æ¸—é€ï¼ŒåŠ é€ŸAIæ›¿ä»£ä¼ ç»Ÿå†…å®¹å¤„ç†å·¥å…·",
                    "timeline": "12-18ä¸ªæœˆå†…ç«äº‰æ ¼å±€å°†è¿›ä¸€æ­¥æ¸…æ™°",
                },
                "funding_logic": {
                    "current_round": "Bè½®",
                    "current_amount": "è¶…10äº¿ç¾å…ƒ",
                    "investors": ["çº¢æ‰ä¸­å›½", "å°çº¢ä¹¦", "é˜¿é‡Œå·´å·´", "ç¾å›¢", "è“é©°åˆ›æŠ•"],
                    "lead_investor": "çº¢æ‰ä¸­å›½ã€å°çº¢ä¹¦",
                    "why_fundable": "åˆ›å§‹äººæ¨æ¤éºŸæ˜¯Transformer-XL/XLNetä½œè€…ï¼Œè®ºæ–‡å¼•ç”¨8000+æ¬¡ï¼Œå­¦æœ¯å½±å“åŠ›é¡¶çº§ï¼›Kimiäº§å“æœˆæ´»ç ´åƒä¸‡éªŒè¯äº†å¸‚åœºéœ€æ±‚ï¼›å¤§æ¨¡å‹èµ›é“æ˜¯å½“å‰ä¸€çº§å¸‚åœºæœ€çƒ­èµ›é“ï¼Œé¡¶çº§VCäº‰ç›¸å¸ƒå±€",
                    "investor_signal": "çº¢æ‰ä¸­å›½è¿ç»­åŠ æ³¨+äº’è”ç½‘å·¨å¤´(é˜¿é‡Œ/ç¾å›¢)æˆ˜ç•¥æŠ•èµ„ï¼Œè¯´æ˜äº§ä¸šç•Œçœ‹å¥½å…¶å•†ä¸šåŒ–æ½œåŠ›",
                    "funding_history": [
                        {"round": "å¤©ä½¿è½®", "amount": "çº¦2000ä¸‡ç¾å…ƒ", "date": "2023å¹´6æœˆ", "investors": ["çº¢æ‰ä¸­å›½"], "lead": "çº¢æ‰ä¸­å›½"},
                        {"round": "Aè½®", "amount": "çº¦10äº¿ç¾å…ƒ", "date": "2023å¹´12æœˆ", "investors": ["å¤šå®¶æœºæ„è”åˆ"], "lead": "æœªæŠ«éœ²"},
                        {"round": "Bè½®", "amount": "è¶…10äº¿ç¾å…ƒ", "date": "2026å¹´1æœˆ", "investors": ["çº¢æ‰ä¸­å›½", "å°çº¢ä¹¦", "é˜¿é‡Œå·´å·´", "ç¾å›¢", "è“é©°åˆ›æŠ•"], "lead": "çº¢æ‰ä¸­å›½ã€å°çº¢ä¹¦"},
                    ],
                },
                "founder_profile": {
                    "name": "æ¨æ¤éºŸ",
                    "background_summary": "æ¸…åCSæœ¬ç§‘â†’CMUåšå£«â†’Google Brainï¼ŒTransformer-XL/XLNetè®ºæ–‡ä¸€ä½œï¼Œå¼•ç”¨è¶…8000æ¬¡",
                    "highlights": [
                        "æ¸…åå¤§å­¦è®¡ç®—æœºç³»æ¯•ä¸š",
                        "å¡å†…åŸºæ¢…éš†å¤§å­¦(CMU)åšå£«ï¼Œå¯¼å¸ˆä¸ºè‹¹æœAIè´Ÿè´£äºº Ruslan Salakhutdinov",
                        "å‰Google Brainå®ä¹ ç ”ç©¶å‘˜",
                        "Transformer-XL/XLNetè®ºæ–‡ä¸€ä½œï¼ˆå¼•ç”¨8000+ï¼‰",
                    ],
                    "rating": "strong",
                },
                "core_tech": {
                    "has_papers": True,
                    "papers": [
                        {"title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "venue": "NeurIPS 2019", "url": "https://scholar.google.com/scholar?q=Transformer-XL", "citations": "~5000"},
                        {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "venue": "NeurIPS 2019", "url": "https://scholar.google.com/scholar?q=XLNet", "citations": "~3000"},
                    ],
                    "open_source_projects": [],
                    "originality": "åŸåˆ›çªç ´",
                    "rating": "cutting_edge",
                },
                "star_team": {
                    "is_star_team": True,
                    "signals": [
                        "åˆ›å§‹äººä¸ºTransformer-XL/XLNetè®ºæ–‡ä¸€ä½œ",
                        "CMUåšå£«+Google BrainèƒŒæ™¯",
                        "æ ¸å¿ƒå›¢é˜Ÿæ¥è‡ªæ¸…å/Google Brain/Meta AI",
                        "å›¢é˜Ÿ200äººè§„æ¨¡",
                    ],
                    "key_members": [
                        {"name": "æ¨æ¤éºŸ", "role": "åˆ›å§‹äºº/CEO", "background": "æ¸…åCSâ†’CMUåšå£«â†’Google Brainï¼ŒTransformer-XL/XLNetä¸€ä½œ"},
                    ],
                    "rating": "all_star",
                },
                "time_sensitivity": "urgent",
                "confidence": 0.92,
            },
            "æ¢çœŸç§‘æŠ€": {
                "title": "æ¢çœŸç§‘æŠ€Aè½®èèµ„ â€” äº‘åŸç”Ÿå®‰å…¨èµ›é“å¡ä½æˆ˜",
                "summary": "æ¢çœŸç§‘æŠ€åˆ‡å…¥K8så®¹å™¨å®‰å…¨ç»†åˆ†èµ›é“ï¼Œåˆ›å§‹äººæœ‰é˜¿é‡Œäº‘å®‰å…¨èƒŒæ™¯ï¼Œç»çº¬é¢†æŠ•çœ‹å¥½äº‘å®‰å…¨å¢é•¿",
                "domain": "cloud",
                "signal_type": "èèµ„äº‹ä»¶",
                "industry_tags": ["äº‘å®‰å…¨", "å®¹å™¨å®‰å…¨", "äº‘åŸç”Ÿ", "Kubernetes"],
                "company_profile": {
                    "name": "æ¢çœŸç§‘æŠ€",
                    "website_url": "",
                    "what_they_do": "æä¾›äº‘åŸç”Ÿç¯å¢ƒä¸‹çš„å®‰å…¨æ£€æµ‹ä¸é˜²æŠ¤ï¼Œæ ¸å¿ƒäº§å“é¢å‘Kubernetesç¯å¢ƒï¼Œè¦†ç›–å®¹å™¨é•œåƒæ‰«æã€è¿è¡Œæ—¶å¨èƒæ£€æµ‹å’Œåˆè§„å®¡è®¡",
                    "product_lines": [
                        {"name": "æ¢çœŸäº‘å«", "description": "é¢å‘K8sç¯å¢ƒçš„å®‰å…¨å¹³å°ï¼Œæä¾›å®¹å™¨é•œåƒæ‰«æã€è¿è¡Œæ—¶å¨èƒæ£€æµ‹å’Œåˆè§„å®¡è®¡", "screenshot_url": "", "metrics": {"users": "50+ä¼ä¸šå®¢æˆ·", "dau": "æœªæŠ«éœ²", "wau": "æœªæŠ«éœ²", "retention": "æœªæŠ«éœ²", "mrr": "é¢„ä¼°ç™¾ä¸‡çº§", "arr": "é¢„ä¼°åƒä¸‡çº§"}},
                    ],
                    "business_model": "SaaSè®¢é˜… + ç§æœ‰åŒ–éƒ¨ç½²",
                    "stage": "early",
                    "contact": {"name": "ææ˜è¾‰ (CEO)", "email": "", "phone": "", "wechat": ""},
                },
                "potential": {
                    "market_size": "ä¸­å›½äº‘å®‰å…¨å¸‚åœº2025å¹´187äº¿å…ƒï¼Œäº‘åŸç”Ÿå®‰å…¨å­èµ›é“é¢„è®¡2027å¹´è¾¾80äº¿å…ƒ",
                    "competitive_advantage": "åˆ›å§‹äººæ·±è€•äº‘å®‰å…¨15å¹´ï¼Œé˜¿é‡Œäº‘èƒŒæ™¯å¸¦æ¥çš„è¡Œä¸šè®¤çŸ¥å’Œå®¢æˆ·èµ„æº",
                    "moat_type": "å…ˆå‘ä¼˜åŠ¿",
                    "potential_rating": "medium",
                    "reasoning": "äº‘åŸç”Ÿå®‰å…¨æ˜¯é«˜å¢é•¿èµ›é“ï¼ˆ25%+ï¼‰ï¼Œä½†ç«äº‰æ¿€çƒˆï¼Œå…¬å¸éœ€è¦åœ¨äº§å“æ·±åº¦ä¸ŠæŒç»­æŠ•å…¥ä»¥å»ºç«‹æŠ€æœ¯å£å’",
                },
                "industry_impact": {
                    "scope": "å±€éƒ¨",
                    "how_it_changes": "æ¨åŠ¨äº‘åŸç”Ÿå®‰å…¨ä»åˆè§„é©±åŠ¨å‘ä¸»åŠ¨é˜²å¾¡æ¼”è¿›ï¼Œæå‡K8sç¯å¢ƒå®‰å…¨åŸºçº¿",
                    "timeline": "6-12ä¸ªæœˆ",
                },
                "funding_logic": {
                    "current_round": "Aè½®",
                    "current_amount": "æ•°åƒä¸‡å…ƒäººæ°‘å¸",
                    "investors": ["ç»çº¬åˆ›æŠ•", "çº¢ç‚¹ä¸­å›½"],
                    "lead_investor": "ç»çº¬åˆ›æŠ•",
                    "why_fundable": "äº‘å®‰å…¨èµ›é“å¢é•¿ç¡®å®šæ€§é«˜ï¼ˆ25%+ï¼‰ï¼Œåˆ›å§‹äººæœ‰é˜¿é‡Œäº‘å®‰å…¨ä¸€çº¿å®æˆ˜ç»éªŒï¼Œ50+ä»˜è´¹å®¢æˆ·éªŒè¯äº†äº§å“ä»·å€¼",
                    "investor_signal": "ç»çº¬åˆ›æŠ•åœ¨å®‰å…¨èµ›é“æœ‰å¤šä¸ªæˆåŠŸæ¡ˆä¾‹ï¼Œé¢†æŠ•è¯´æ˜å¯¹äº‘å®‰å…¨æ–¹å‘æŒç»­çœ‹å¥½",
                    "funding_history": [
                        {"round": "å¤©ä½¿è½®", "amount": "æœªæŠ«éœ²", "date": "2024å¹´åˆ", "investors": ["çº¢ç‚¹ä¸­å›½"], "lead": "çº¢ç‚¹ä¸­å›½"},
                        {"round": "Aè½®", "amount": "æ•°åƒä¸‡å…ƒäººæ°‘å¸", "date": "2026å¹´1æœˆ", "investors": ["ç»çº¬åˆ›æŠ•", "çº¢ç‚¹ä¸­å›½"], "lead": "ç»çº¬åˆ›æŠ•"},
                    ],
                },
                "founder_profile": {
                    "name": "ææ˜è¾‰",
                    "background_summary": "15å¹´å®‰å…¨è€å…µï¼Œå‰é˜¿é‡Œäº‘å®‰å…¨äº§å“çº¿è´Ÿè´£äººï¼Œå‰ç»¿ç›Ÿç§‘æŠ€é«˜çº§ç ”ç©¶å‘˜",
                    "highlights": [
                        "15å¹´ç½‘ç»œå®‰å…¨ä»ä¸šç»éªŒ",
                        "å‰é˜¿é‡Œäº‘å®‰å…¨äº§å“çº¿è´Ÿè´£äºº",
                        "å‰ç»¿ç›Ÿç§‘æŠ€é«˜çº§ç ”ç©¶å‘˜",
                    ],
                    "rating": "strong",
                },
                "core_tech": {
                    "has_papers": True,
                    "papers": [
                        {"title": "CTOç‹ç£Šåœ¨IEEE S&P/USENIX Securityå‘è¡¨å¤šç¯‡å®‰å…¨é¢†åŸŸè®ºæ–‡", "venue": "IEEE S&P / USENIX Security", "url": "", "citations": ""},
                    ],
                    "open_source_projects": [],
                    "originality": "å·¥ç¨‹åˆ›æ–°",
                    "rating": "solid",
                },
                "star_team": {
                    "is_star_team": False,
                    "signals": [
                        "åˆ›å§‹äººå‰é˜¿é‡Œäº‘å®‰å…¨äº§å“çº¿è´Ÿè´£äºº",
                        "CTOæ¸…åç½‘ç»œå®‰å…¨åšå£«+é¡¶ä¼šè®ºæ–‡",
                    ],
                    "key_members": [
                        {"name": "ææ˜è¾‰", "role": "åˆ›å§‹äºº/CEO", "background": "15å¹´å®‰å…¨ç»éªŒï¼Œå‰é˜¿é‡Œäº‘å®‰å…¨äº§å“çº¿è´Ÿè´£äºº"},
                        {"name": "ç‹ç£Š", "role": "CTO", "background": "æ¸…åå¤§å­¦ç½‘ç»œå®‰å…¨åšå£«ï¼ŒIEEE S&P/USENIX Securityè®ºæ–‡ä½œè€…"},
                    ],
                    "rating": "strong",
                },
                "time_sensitivity": "short_term",
                "confidence": 0.78,
            },
            "FlowAgent": {
                "title": "FlowAgentç§å­è½® â€” AI Agentå·¥ä½œæµç¼–æ’çš„æ—©æœŸå…¥åœºè€…",
                "summary": "å‰OpenAIå·¥ç¨‹å¸ˆåˆ›ä¸šåšAgentç¼–æ’å¹³å°ï¼ŒReActè®ºæ–‡åŠ æŒï¼Œèµ›é“æ­£çƒ­ä½†ç«äº‰æ¿€çƒˆ",
                "domain": "ai",
                "signal_type": "èèµ„äº‹ä»¶",
                "industry_tags": ["AI Agent", "å·¥ä½œæµè‡ªåŠ¨åŒ–", "ä½ä»£ç ", "å¤§æ¨¡å‹åº”ç”¨"],
                "company_profile": {
                    "name": "FlowAgent",
                    "website_url": "",
                    "what_they_do": "ä½ä»£ç AI Agentç¼–æ’å¹³å°ï¼Œè®©ä¼ä¸šç”¨æˆ·é€šè¿‡æ‹–æ‹½æ–¹å¼æ„å»ºå¤æ‚AIå·¥ä½œæµï¼Œæ”¯æŒå¤šæ¨¡å‹è°ƒåº¦ã€å·¥å…·è°ƒç”¨å’Œäººæœºåä½œ",
                    "product_lines": [
                        {"name": "FlowAgent Platform", "description": "ä½ä»£ç AI Agentç¼–æ’å¹³å°ï¼Œæ‹–æ‹½æ„å»ºå·¥ä½œæµï¼Œæ”¯æŒå¤šæ¨¡å‹è°ƒåº¦", "screenshot_url": "", "metrics": {"users": "200å®¶ä¼ä¸šå†…æµ‹ç”³è¯·", "dau": "å†…æµ‹é˜¶æ®µæœªå…¬å¼€", "wau": "å†…æµ‹é˜¶æ®µæœªå…¬å¼€", "retention": "æœªæŠ«éœ²", "mrr": "å°šæœªå•†ä¸šåŒ–", "arr": "å°šæœªå•†ä¸šåŒ–"}},
                        {"name": "FlowEngine", "description": "å¼€æºæ ¸å¿ƒæ¨ç†æ¡†æ¶ï¼Œæ”¯æŒReAct/CoTç­‰æ¨ç†æ¨¡å¼", "screenshot_url": "", "metrics": {"users": "GitHub 3.2k Stars", "dau": "æœªæŠ«éœ²", "wau": "æœªæŠ«éœ²", "retention": "æœªæŠ«éœ²", "mrr": "å¼€æºå…è´¹", "arr": "å¼€æºå…è´¹"}},
                    ],
                    "business_model": "SaaSè®¢é˜…ï¼ˆæŒ‰å·¥ä½œæµè°ƒç”¨é‡è®¡è´¹ï¼‰",
                    "stage": "seed",
                    "contact": {"name": "å¼ æ¶µ (CEO)", "email": "", "phone": "", "wechat": ""},
                },
                "potential": {
                    "market_size": "å…¨çƒAI Agentå¸‚åœº2025å¹´èèµ„è¶…50äº¿ç¾å…ƒï¼Œä¼ä¸šçº§Agentå¹³å°èµ›é“å¤„äºæ—©æœŸçˆ†å‘é˜¶æ®µ",
                    "competitive_advantage": "åˆ›å§‹äººOpenAIèƒŒæ™¯+ReActè®ºæ–‡å¼•ç”¨2000+ï¼Œå¯¹AgentæŠ€æœ¯æœ‰æ·±åˆ»ç†è§£ï¼›å¼€æºç­–ç•¥ç§¯ç´¯å¼€å‘è€…ç”Ÿæ€",
                    "moat_type": "æŠ€æœ¯å£å’",
                    "potential_rating": "high",
                    "reasoning": "AI Agentè¢«è§†ä¸ºå¤§æ¨¡å‹æœ€é‡è¦çš„åº”ç”¨èŒƒå¼ï¼Œèµ›é“å¤©èŠ±æ¿æé«˜ï¼›ä½†å½“å‰å¤„äºææ—©æœŸï¼Œäº§å“èƒ½å¦è·‘å‡ºæ¥å–å†³äºæ‰§è¡ŒåŠ›",
                },
                "industry_impact": {
                    "scope": "é‡å¤§",
                    "how_it_changes": "é™ä½ä¼ä¸šæ„å»ºAIè‡ªåŠ¨åŒ–å·¥ä½œæµçš„é—¨æ§›ï¼ŒåŠ é€ŸAIä»'å¯¹è¯åŠ©æ‰‹'å‘'è‡ªä¸»æ‰§è¡Œä»»åŠ¡'çš„èŒƒå¼è½¬ç§»",
                    "timeline": "18-24ä¸ªæœˆ",
                },
                "funding_logic": {
                    "current_round": "ç§å­è½®",
                    "current_amount": "500ä¸‡ç¾å…ƒ",
                    "investors": ["çœŸæ ¼åŸºé‡‘", "å¥‡ç»©åˆ›å›"],
                    "lead_investor": "çœŸæ ¼åŸºé‡‘",
                    "why_fundable": "åˆ›å§‹äººæœ‰OpenAI+æ–¯å¦ç¦èƒŒæ™¯ï¼ŒReActè®ºæ–‡æ˜¯Agenté¢†åŸŸå¥ åŸºæ€§å·¥ä½œï¼›AI Agentèµ›é“æ­£çƒ­ï¼ŒæŠ•èµ„äººæŠ¢è·‘å¸ƒå±€æ—©æœŸé¡¹ç›®",
                    "investor_signal": "çœŸæ ¼+å¥‡ç»©æ˜¯å…¸å‹å¤©ä½¿/ç§å­è½®å¼ºåŠ¿æŠ•èµ„æ–¹ï¼Œè¯´æ˜å¯¹åˆ›å§‹äººä¸ªäººèƒ½åŠ›é«˜åº¦è®¤å¯",
                    "funding_history": [
                        {"round": "ç§å­è½®", "amount": "500ä¸‡ç¾å…ƒ", "date": "2026å¹´2æœˆ", "investors": ["çœŸæ ¼åŸºé‡‘", "å¥‡ç»©åˆ›å›"], "lead": "çœŸæ ¼åŸºé‡‘"},
                    ],
                },
                "founder_profile": {
                    "name": "å¼ æ¶µ",
                    "background_summary": "åŒ—å¤§æœ¬ç§‘â†’æ–¯å¦ç¦AI Labåšå£«â†’OpenAIç ”ç©¶å·¥ç¨‹å¸ˆï¼Œå‚ä¸GPT-4è®­ç»ƒï¼ŒReActè®ºæ–‡å¼•ç”¨2000+",
                    "highlights": [
                        "åŒ—äº¬å¤§å­¦è®¡ç®—æœºç³»æœ¬ç§‘",
                        "æ–¯å¦ç¦å¤§å­¦AI Labåšå£«",
                        "å‰OpenAIç ”ç©¶å·¥ç¨‹å¸ˆï¼ˆå‚ä¸GPT-4 RLHFè®­ç»ƒï¼‰",
                        "ReActè®ºæ–‡å¼•ç”¨è¶…2000æ¬¡",
                    ],
                    "rating": "strong",
                },
                "core_tech": {
                    "has_papers": True,
                    "papers": [
                        {"title": "ReAct: Synergizing Reasoning and Acting in Language Models", "venue": "ICLR 2023", "url": "https://scholar.google.com/scholar?q=ReAct+Synergizing+Reasoning+Acting", "citations": "2000+"},
                    ],
                    "open_source_projects": [
                        {"name": "FlowEngine", "url": "https://github.com/flowagent/flowengine", "stars": "3.2k"},
                    ],
                    "originality": "åŸåˆ›çªç ´",
                    "rating": "cutting_edge",
                },
                "star_team": {
                    "is_star_team": True,
                    "signals": [
                        "åˆ›å§‹äººå‰OpenAIç ”ç©¶å·¥ç¨‹å¸ˆ",
                        "æ–¯å¦ç¦AI Labåšå£«",
                        "ReActè®ºæ–‡å¼•ç”¨2000+",
                        "è”åˆåˆ›å§‹äººå‰å­—èŠ‚é£ä¹¦æŠ€æœ¯è´Ÿè´£äºº",
                    ],
                    "key_members": [
                        {"name": "å¼ æ¶µ", "role": "åˆ›å§‹äºº/CEO", "background": "åŒ—å¤§â†’æ–¯å¦ç¦AI Labåšå£«â†’å‰OpenAIç ”ç©¶å·¥ç¨‹å¸ˆ"},
                        {"name": "åˆ˜æ€è¿œ", "role": "è”åˆåˆ›å§‹äºº/CTO", "background": "å‰å­—èŠ‚è·³åŠ¨é£ä¹¦å›¢é˜ŸæŠ€æœ¯è´Ÿè´£äººï¼Œä¼ä¸šçº§SaaSä¸“å®¶"},
                    ],
                    "rating": "all_star",
                },
                "time_sensitivity": "urgent",
                "confidence": 0.82,
            },
        }
        return mocks.get(company, self._generic_mock(item))

    def _generic_mock(self, item: dict) -> dict:
        return {
            "title": f"{item.get('funding_company', 'æœªçŸ¥')} â€” å•†æœºåˆ†æ",
            "summary": item.get("summary", "å¾…åˆ†æ"),
            "domain": item.get("primary_category", "ai"),
            "signal_type": item.get("signal_type", "å…¶ä»–"),
            "company_profile": {"name": item.get("funding_company", ""), "what_they_do": item.get("company_desc", ""), "products": [], "business_model": "æœªçŸ¥", "stage": "early"},
            "potential": {"market_size": "å¾…è°ƒç ”", "competitive_advantage": "å¾…åˆ†æ", "moat_type": "æ— æ˜æ˜¾æŠ¤åŸæ²³", "potential_rating": "medium", "reasoning": "ä¿¡æ¯ä¸è¶³"},
            "industry_impact": {"scope": "ä¸­ç­‰", "how_it_changes": "å¾…åˆ†æ", "timeline": "å¾…è¯„ä¼°"},
            "funding_logic": {"round": item.get("funding_round", ""), "amount": item.get("funding_amount", ""), "investors": item.get("investors", []), "why_fundable": "å¾…åˆ†æ", "investor_signal": "å¾…åˆ†æ"},
            "founder_profile": {"name": item.get("founder_name", ""), "background_summary": "å¾…è°ƒç ”", "highlights": item.get("founder_clues", []), "rating": "unknown"},
            "core_tech": {"has_papers": False, "key_papers": [], "open_source": [], "originality": "å¾…è¯„ä¼°", "rating": "average"},
            "star_team": {"is_star_team": False, "signals": [], "rating": "unknown"},
            "time_sensitivity": "medium_term",
            "confidence": 0.5,
        }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Agent 5: Evaluator (ä¸ƒç»´è¯„åˆ†æ’å) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class EvaluatorAgent(BaseAgent):
    name = "Evaluator"
    emoji = "âš–ï¸"

    # æƒé‡é…ç½®
    WEIGHTS = {
        "company": 0.10,
        "potential": 0.20,
        "impact": 0.15,
        "funding": 0.20,
        "founder": 0.15,
        "tech": 0.10,
        "team": 0.10,
    }

    async def run(self, opportunities: list[dict]) -> list[dict]:
        # å¹¶å‘è¯„åˆ†
        tasks = [self._score(opp) for opp in opportunities]
        scored = list(await asyncio.gather(*tasks))
        # æ’åº
        scored.sort(key=lambda x: x.get("scores", {}).get("total", 0), reverse=True)
        for i, opp in enumerate(scored):
            opp["rank"] = i + 1
        return scored

    async def _score(self, opp: dict) -> dict:
        system_prompt = """ä½ æ˜¯å•†æœºè¯„åˆ†ä¸“å®¶ã€‚è¯·åªè¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¸è¦ä»»ä½•è§£é‡Šæ–‡å­—ã€‚æ ¼å¼:
{"scores":{"company":8,"potential":9,"impact":7,"funding":9,"founder":9,"tech":10,"team":9,"total":8.8},"importance_level":"S","special_tag":"all_rounder|tech_dark_horse|capital_darling|none","one_line_verdict":"ä¸€å¥è¯è¯„è¯­"}
æ¯ç»´åº¦1-10åˆ†ã€‚total=company*0.1+potential*0.2+impact*0.15+funding*0.2+founder*0.15+tech*0.1+team*0.1
Sçº§(>=8.0) Açº§(6.5-7.9) Bçº§(5.0-6.4) Cçº§(<5.0)"""

        # ç²¾ç®€è¾“å…¥ï¼Œåªä¼ å…³é”®ä¿¡æ¯
        summary = {
            "title": opp.get("title", ""),
            "company": opp.get("company_profile", {}).get("name", ""),
            "what": opp.get("company_profile", {}).get("what_they_do", "")[:100],
            "funding": f"{opp.get('funding_logic', {}).get('round', '')} {opp.get('funding_logic', {}).get('amount', '')}",
            "investors": opp.get("funding_logic", {}).get("investors", []),
            "founder": opp.get("founder_profile", {}).get("background_summary", ""),
            "tech": opp.get("core_tech", {}).get("rating", ""),
            "papers": opp.get("core_tech", {}).get("has_papers", False),
            "star_team": opp.get("star_team", {}).get("is_star_team", False),
            "potential": opp.get("potential", {}).get("potential_rating", ""),
            "impact": opp.get("industry_impact", {}).get("scope", ""),
        }
        user_prompt = json.dumps(summary, ensure_ascii=False)
        result = await call_llm(system_prompt, user_prompt)

        if not result:
            result = self._mock_score(opp)

        opp["scores"] = result.get("scores", {})
        opp["importance_level"] = result.get("importance_level", "B")
        opp["special_tag"] = result.get("special_tag", "none")
        opp["one_line_verdict"] = result.get("one_line_verdict", "")
        return opp

    def _mock_score(self, opp: dict) -> dict:
        company = opp.get("company_profile", {}).get("name", "")
        mocks = {
            "æœˆä¹‹æš—é¢ (Moonshot AI)": {
                "scores": {"company": 9, "potential": 9, "impact": 8, "funding": 10, "founder": 10, "tech": 10, "team": 9, "total": 9.25},
                "importance_level": "S",
                "special_tag": "all_rounder",
                "one_line_verdict": "å…¨æ˜æ˜Ÿå›¢é˜Ÿ+å·¨é¢èèµ„+é¡¶çº§æŠ€æœ¯ï¼Œå›½äº§å¤§æ¨¡å‹èµ›é“å¤´éƒ¨æ ‡çš„",
            },
            "æ¢çœŸç§‘æŠ€": {
                "scores": {"company": 7, "potential": 7, "impact": 5, "funding": 6, "founder": 7, "tech": 6, "team": 6, "total": 6.40},
                "importance_level": "B",
                "special_tag": "none",
                "one_line_verdict": "äº‘å®‰å…¨å¢é•¿èµ›é“çš„åŠ¡å®é€‰æ‰‹ï¼Œåˆ›å§‹äººè¡Œä¸šç»éªŒä¸°å¯Œä½†å·®å¼‚åŒ–æœ‰å¾…åŠ å¼º",
            },
            "FlowAgent": {
                "scores": {"company": 7, "potential": 9, "impact": 8, "funding": 7, "founder": 9, "tech": 9, "team": 8, "total": 8.15},
                "importance_level": "S",
                "special_tag": "tech_dark_horse",
                "one_line_verdict": "OpenAI+ReActè®ºæ–‡èƒŒæ™¯çš„Agentåˆ›ä¸šè€…ï¼ŒæŠ€æœ¯åº•è•´æ·±åšï¼Œèµ›é“æ­£å¤„çˆ†å‘å‰å¤œ",
            },
        }
        return mocks.get(company, {
            "scores": {"company": 5, "potential": 5, "impact": 5, "funding": 5, "founder": 5, "tech": 5, "team": 5, "total": 5.0},
            "importance_level": "B",
            "special_tag": "none",
            "one_line_verdict": "å¾…è¿›ä¸€æ­¥åˆ†æ",
        })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Agent 6: Reporter (å›¾æ–‡æŠ¥å‘Šç”Ÿæˆ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class ReporterAgent(BaseAgent):
    name = "Reporter"
    emoji = "ğŸ“Š"

    async def run(self, opportunities: list[dict]) -> list[dict]:
        reports = []
        for opp in opportunities:
            report = self._generate_report(opp)
            reports.append(report)
        return reports

    def _generate_report(self, opp: dict) -> dict:
        scores = opp.get("scores", {})
        cp = opp.get("company_profile", {})
        pot = opp.get("potential", {})
        imp = opp.get("industry_impact", {})
        fl = opp.get("funding_logic", {})
        fp = opp.get("founder_profile", {})
        ct = opp.get("core_tech", {})
        st = opp.get("star_team", {})

        level = opp.get("importance_level", "B")
        level_colors = {"S": "#dc2626", "A": "#ea580c", "B": "#ca8a04", "C": "#6b7280"}
        level_labels = {"S": "Sçº§ Â· é‡å¤§æœºä¼š", "A": "Açº§ Â· é«˜ä»·å€¼", "B": "Bçº§ Â· æœ‰æ½œåŠ›", "C": "Cçº§ Â· ä½ä¼˜å…ˆ"}

        tag_map = {
            "tech_dark_horse": "ğŸ´ æŠ€æœ¯é»‘é©¬",
            "capital_darling": "ğŸ’° èµ„æœ¬å® å„¿",
            "all_rounder": "â­ å…¨èƒ½é€‰æ‰‹",
            "none": "",
        }
        special = tag_map.get(opp.get("special_tag", "none"), "")

        # æå–äº§å“ååˆ—è¡¨ï¼ˆå…¼å®¹æ–°æ—§æ ¼å¼ï¼‰
        product_names = [p.get("name", p) if isinstance(p, dict) else p for p in cp.get("product_lines", cp.get("products", []))]

        # ç”Ÿæˆ Mermaid æµç¨‹å›¾ä»£ç 
        mermaid_code = f"""graph LR
    A["{cp.get('name', 'å…¬å¸')}"] --> B["æ ¸å¿ƒäº§å“"]
    B --> C["{', '.join(product_names[:2])}"]
    A --> D["èèµ„"]
    D --> E["{fl.get('current_round', fl.get('round', '?'))} {fl.get('current_amount', fl.get('amount', '?'))}"]
    A --> F["æŠ€æœ¯"]
    F --> G["{ct.get('originality', '?')}"]"""

        # æ„å»ºé›·è¾¾å›¾æ•°æ®
        radar_data = {
            "labels": ["å…¬å¸ä¸šåŠ¡", "å¸‚åœºæ½œåŠ›", "è¡Œä¸šå½±å“", "èèµ„é€»è¾‘", "åˆ›å§‹äºº", "æ ¸å¿ƒæŠ€æœ¯", "å›¢é˜Ÿ"],
            "values": [
                scores.get("company", 5),
                scores.get("potential", 5),
                scores.get("impact", 5),
                scores.get("funding", 5),
                scores.get("founder", 5),
                scores.get("tech", 5),
                scores.get("team", 5),
            ],
        }

        # â”€â”€ ä»è”ç½‘æœç´¢è¡¥å……äº§å“å›¾ç‰‡ï¼ˆè¿‡æ»¤æ‰ logo å’Œä¸ç›¸å…³å›¾ç‰‡ï¼‰â”€â”€
        web_images = opp.get("web_product_images", [])
        # è¿‡æ»¤: è·³è¿‡ title ä¸­å« logo/å›¾æ ‡/icon çš„å›¾ç‰‡ï¼Œä»¥åŠå°ºå¯¸å¤ªå°çš„å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯iconï¼‰
        logo_keywords = ["logo", "å›¾æ ‡", "icon", "favicon", "avatar", "å¤´åƒ"]
        filtered_images = [
            img for img in web_images
            if not any(kw in (img.get("title", "") or "").lower() for kw in logo_keywords)
            and img.get("width", 999) > 100 and img.get("height", 999) > 100
        ]
        product_lines = cp.get("product_lines", [])
        img_idx = 0
        for pl in product_lines:
            if isinstance(pl, dict) and not pl.get("screenshot_url") and img_idx < len(filtered_images):
                pl["screenshot_url"] = filtered_images[img_idx].get("url", "")
                pl["screenshot_title"] = filtered_images[img_idx].get("title", "")
                img_idx += 1
        extra_images = filtered_images[img_idx:]

        # â”€â”€ ä»è”ç½‘æœç´¢è¡¥å……è®ºæ–‡é“¾æ¥ â”€â”€
        # ä» web_paper_content ä¸­æå– arxiv é“¾æ¥
        paper_content = opp.get("web_paper_content", "")
        import re
        arxiv_links = re.findall(r'https?://arxiv\.org/[^\s\]ï¼‰)]+', paper_content)
        papers = ct.get("papers", ct.get("key_papers", []))
        for i, p in enumerate(papers):
            if isinstance(p, dict) and not p.get("url") and i < len(arxiv_links):
                p["url"] = arxiv_links[i]
        # å¦‚æœ papers æ˜¯æ—§æ ¼å¼ï¼ˆå­—ç¬¦ä¸²åˆ—è¡¨ï¼‰ï¼Œè½¬ä¸ºå¸¦é“¾æ¥çš„ç»“æ„
        if papers and isinstance(papers[0], str):
            new_papers = []
            for i, title in enumerate(papers):
                new_papers.append({
                    "title": title,
                    "venue": "",
                    "url": arxiv_links[i] if i < len(arxiv_links) else "",
                    "citations": "",
                })
            ct["papers"] = new_papers

        # â”€â”€ ä»è”ç½‘æœç´¢çš„ references è¡¥å……è®ºæ–‡é“¾æ¥ï¼ˆå¦‚æœ arxiv ä¸å¤Ÿï¼‰â”€â”€
        paper_refs = opp.get("web_paper_references", [])
        for p in papers:
            if isinstance(p, dict) and not p.get("url"):
                # å°è¯•ä» references ä¸­åŒ¹é…
                for ref in paper_refs:
                    if ref.get("url") and ("arxiv" in ref.get("url", "") or "scholar" in ref.get("url", "")):
                        p["url"] = ref["url"]
                        break

        return {
            "opportunity_id": opp.get("id", ""),
            "title": opp.get("title", ""),
            "level": level,
            "level_color": level_colors.get(level, "#6b7280"),
            "level_label": level_labels.get(level, ""),
            "special_tag": special,
            "total_score": scores.get("total", 0),
            "one_line_verdict": opp.get("one_line_verdict", ""),
            "industry_tags": opp.get("industry_tags", []),
            "company_profile": cp,
            "potential": pot,
            "industry_impact": imp,
            "funding_logic": fl,
            "founder_profile": fp,
            "core_tech": ct,
            "star_team": st,
            "scores": scores,
            "radar_data": radar_data,
            "mermaid_code": mermaid_code,
            "extra_images": extra_images[:3],
            "source_article_id": opp.get("source_article_id", ""),
            "created_at": opp.get("created_at", ""),
        }


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                       Pipeline ç¼–æ’                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class Pipeline:
    def __init__(self):
        self.crawler = CrawlerAgent()
        self.cleaner = CleanerAgent()
        self.integrator = IntegratorAgent()
        self.analyst = AnalystAgent()
        self.evaluator = EvaluatorAgent()
        self.reporter = ReporterAgent()

    async def run(self):
        """æ‰§è¡Œå®Œæ•´æµæ°´çº¿ï¼Œyield SSE äº‹ä»¶ï¼ˆå¸¦å¿ƒè·³ä¿æ´»ï¼‰"""
        agents = [
            ("crawler", self.crawler),
            ("cleaner", self.cleaner),
            ("integrator", self.integrator),
            ("analyst", self.analyst),
            ("evaluator", self.evaluator),
            ("reporter", self.reporter),
        ]

        data = None
        for i, (name, agent) in enumerate(agents):
            yield self._event("agent_start", {"agent": name, "emoji": agent.emoji, "step": i + 1, "total": len(agents)})
            try:
                start = time.time()
                # ç”¨å¿ƒè·³åŒ…è£¹é•¿æ—¶é—´è¿è¡Œçš„ agent
                task = asyncio.create_task(agent.run(data))
                while not task.done():
                    await asyncio.sleep(3)
                    if not task.done():
                        elapsed_so_far = round(time.time() - start, 1)
                        yield self._event("heartbeat", {"agent": name, "elapsed": elapsed_so_far})
                data = task.result()
                elapsed = round(time.time() - start, 2)
                count = len(data) if isinstance(data, list) else 1
                yield self._event("agent_done", {"agent": name, "emoji": agent.emoji, "elapsed": elapsed, "output_count": count})
            except Exception as e:
                yield self._event("agent_error", {"agent": name, "error": str(e)})
                return

        # å­˜å‚¨ç»“æœ
        db["articles"] = SAMPLE_ARTICLES
        db["cleaned"] = data  # reporter output is the final data
        db["opportunities"] = data
        db["reports"] = data

        yield self._event("pipeline_done", {
            "total_opportunities": len(data),
            "s_count": sum(1 for r in data if r.get("level") == "S"),
            "a_count": sum(1 for r in data if r.get("level") == "A"),
        })

    def _event(self, event_type: str, data: dict) -> str:
        return f"event: {event_type}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                        FastAPI åº”ç”¨                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@asynccontextmanager
async def lifespan(app: FastAPI):
    print(f"\n{'='*60}")
    print(f"  ğŸ’° ä»€ä¹ˆå€¼å¾—æŠ• â€” å¤šæ™ºèƒ½ä½“å•†æœºå‘æ˜ Demo")
    print(f"  ğŸŒ è®¿é—®åœ°å€: http://localhost:8000")
    print(f"  ğŸ¤– LLM æ¨¡å¼: {'Mockï¼ˆæ— éœ€API Keyï¼‰' if USE_MOCK else f'ç«å±±æ–¹èˆŸ ({OPENAI_MODEL})'}")
    print(f"{'='*60}\n")
    yield

app = FastAPI(title="ä»€ä¹ˆå€¼å¾—æŠ•", lifespan=lifespan)
app.mount("/static", StaticFiles(directory="static"), name="static")
pipeline = Pipeline()


@app.get("/", response_class=HTMLResponse)
async def index():
    with open("static/index.html", "r", encoding="utf-8") as f:
        return f.read()


@app.get("/api/status")
async def status():
    return {
        "mode": "mock" if USE_MOCK else "llm",
        "model": OPENAI_MODEL if not USE_MOCK else "mock",
        "articles_count": len(db["articles"]),
        "opportunities_count": len(db["opportunities"]),
    }


@app.get("/api/articles")
async def get_articles():
    return SAMPLE_ARTICLES


@app.get("/api/opportunities")
async def get_opportunities():
    return db.get("reports", [])


@app.get("/api/run")
async def run_pipeline():
    """SSE ç«¯ç‚¹: è¿è¡Œå®Œæ•´æµæ°´çº¿"""
    async def event_stream():
        async for event in pipeline.run():
            yield event
    return StreamingResponse(event_stream(), media_type="text/event-stream")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                          å¯åŠ¨å…¥å£                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True)
